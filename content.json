[{"title":"在K8S集群中搭建Redis集群(istio网络).md","date":"2020-03-25T06:46:50.000Z","path":"2020/03/25/DevOps/在K8S集群中搭建Redis集群/","text":"背景 在 K8S 集群中搭建 Redis 集群 K8S 集群使用 istio 接管网络 注意事项 由于 istio 接管网络，会造成 Redis 集群初始化不能成功（一直卡在：Waiting for the cluster to join） 以上问题需要在 pod 启动的时候设置 cluster-announce-ip，具体见下 ↓ 部署集群前提 使用 PV/PVC 存储（可搭建 NFS） 使用 StatefulSet 配置集群 确认节点间 6379、16379 端口可访问 部署 部署 NFS 管理(nfs-client-provisioner) cat &lt;&lt; EOF &gt; ncp.yaml --- kind: ServiceAccount apiVersion: v1 metadata: name: nfs-client-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumeclaims&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;] - apiGroups: [&quot;storage.k8s.io&quot;] resources: [&quot;storageclasses&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;endpoints&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Deployment apiVersion: apps/v1 metadata: name: nfs-client-provisioner spec: replicas: 1 selector: matchLabels: app: nfs-client-provisioner strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: &lt;镜像仓库(网络方便可以自行到 Docker HUB 找镜像、版本)&gt;/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: nfs-client-provisioner - name: NFS_SERVER value: &lt;NFS 服务端主机 IP&gt; - name: NFS_PATH value: &lt;NSF 服务端导出目录&gt; volumes: - name: nfs-client-root nfs: server: &lt;NFS 服务端主机 IP&gt; path: &lt;NSF 服务端导出目录&gt; --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-storage provisioner: nfs-client-provisioner EOF 部署 Redis 集群 cat &lt;&lt; EOF &gt; redis-cluster.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: redis-cluster data: update-node.sh: | #!/bin/sh REDIS_NODES=&quot;/data/nodes.conf&quot; sed -i -e &quot;/myself/ s/[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}/${POD_IP}/&quot; ${REDIS_NODES} cp /conf/redis.conf /redis.conf sed -i &quot;s/MY_IP/${POD_IP}/&quot; /redis.conf # 解决 istio 网络下，Redis 集群初始化失败问题 exec &quot;$@&quot; redis.conf: |+ cluster-enabled yes cluster-require-full-coverage no cluster-node-timeout 15000 cluster-config-file /data/nodes.conf cluster-migration-barrier 1 appendonly yes protected-mode no cluster-announce-ip MY_IP # 解决 istio 网络下，Redis 集群初始化失败问题 #创建statefulset服务 --- apiVersion: apps/v1 kind: StatefulSet metadata: name: redis-cluster spec: serviceName: redis-cluster replicas: 6 selector: matchLabels: app: redis-cluster template: metadata: labels: app: redis-cluster spec: containers: - name: redis image: &lt;镜像仓库(网络方便可以自行到 Docker HUB 找镜像、版本&gt;/redis:5.0.1-alpine ports: - containerPort: 6379 name: client - containerPort: 16379 name: gossip command: [&quot;/conf/update-node.sh&quot;, &quot;redis-server&quot;, &quot;/redis.conf&quot;] env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP volumeMounts: - name: conf mountPath: /conf readOnly: false - name: data mountPath: /data readOnly: false volumes: - name: conf configMap: name: redis-cluster defaultMode: 0755 volumeClaimTemplates: # pvc模板 - metadata: name: data annotations: volume.beta.kubernetes.io/storage-class: nfs-storage # 指定存储类为nfs-storage spec: accessModes: [ &quot;ReadWriteOnce&quot; ] resources: requests: storage: 1Gi # 指定持久卷大小 --- apiVersion: v1 kind: Service metadata: name: redis-cluster spec: type: ClusterIP ports: - port: 6379 targetPort: 6379 name: client - port: 16379 targetPort: 16379 name: gossip selector: app: redis-cluster 初始化集群 $kubectl exec -it redis-cluster-0 -- redis-cli --cluster create --cluster-replicas 1 $(kubectl get pods -l app=redis-cluster -o jsonpath='{range.items[*]}{.status.podIP}:6379 ') 测试 使用redis-cli -c测试集群命令 参考链接 Deploying Redis Cluster on Top of Kubernetes Is it possible to create a Redis Cluster within Kubernetes using a Istio Search Mesh?","tags":[{"name":"K8S","slug":"K8S","permalink":"http://www.lockey.xyz/tags/K8S/"},{"name":"istio","slug":"istio","permalink":"http://www.lockey.xyz/tags/istio/"},{"name":"Redis","slug":"Redis","permalink":"http://www.lockey.xyz/tags/Redis/"},{"name":"集群","slug":"集群","permalink":"http://www.lockey.xyz/tags/%E9%9B%86%E7%BE%A4/"}]},{"title":"Ubuntu18.04 NFS 安装使用","date":"2020-03-25T06:12:31.000Z","path":"2020/03/25/Linux/Ubuntu18.04--NFS安装使用/","text":"环境 Ubuntu 18.04 安装 安装软件包 $apt install nfs-kernel-server 如出现依赖错误 ↓ . . . The following packages have unmet dependencies: nfs-kernel-server : Depends: libtirpc1 but it is not going to be installed Depends: nfs-common (= 1:1.2.8-6ubuntu1.2) but it is not going to be installed E: Unable to correct problems, you have held broken packages. 错误解决办法： apt 安装依赖 第一步如果出现更多依赖报错看第 3 步 手动安装 下载安装包（libtirpc1_0.2.5-1.2ubuntu0.1_amd64.deb）nfs-kernel-server_1.3.4-2.1ubuntu5_amd64.debnfs-common_1.3.4-2.1ubuntu5_amd64.debkeyutils_1.5.9-9.2ubuntu2_amd64.deb 使用dpkg -i安装下载好的 deb 包；如出现错误提示，可以根据提示使用 apt 或再下载安装包安装 再有问题可以使用 journalctl 等工具排查问题 配置 创建导出目录（服务端、客户端共享目录）$mkdir -p /xxx/xxxxx 开放权限$chown nobody:nogroup /xxx/xxxxx &amp;&amp; chmod 777 /xxx/xxxxx(权限开放自己控制，全开有风险，需谨慎使用) 配置导出目录权限$vim /etc/exports(具体规则 exports 里有 example) 重启 nfs-kernel-server、rpcbind 服务 使用 客户端主机需要安装 nfs-common 安装方法同上 在客户端主机创建挂载目录 $mkdir -p /xxx/xxxxx 挂载 $mount -t nfs &lt;NFS服务端主机IP&gt;:&lt;NFS服务端主机导出目录&gt; &lt;客户端主机挂载目录&gt; 测试 在客户端挂载目录中操作（创建文件、文件夹等），能在服务端导出目录中同步 在服务端导出目录中操作（创建文件、文件夹等），能在客户端挂载目录中同步","tags":[{"name":"NFS","slug":"NFS","permalink":"http://www.lockey.xyz/tags/NFS/"},{"name":"nfs-kernel-server","slug":"nfs-kernel-server","permalink":"http://www.lockey.xyz/tags/nfs-kernel-server/"},{"name":"Depends","slug":"Depends","permalink":"http://www.lockey.xyz/tags/Depends/"}]},{"title":"istio 安装使用","date":"2020-03-16T08:06:48.000Z","path":"2020/03/16/DevOps/istio安装使用/","text":"概念 Istio 是Google、IBM 和Lyft 联合开源的服务网格（Service Mesh）框架，旨在解决大量微服务的发现、连接、管理、监控以及安全等问题。 Istio 对应用是透明的，不需要改动任何服务代码就可以实现透明的服务治理。 官方文档（中英文可切换） istio 注意 官方文档实在太全了，没什么好整理的 稍微阅读文档里的“概念部分” 安装istio 推荐使用：istioctl；helm 方式在新版本已经弃用 istio 默认使用国外镜像仓库，可以自行修改配置文件（install &gt; kubenetes） 安装示例走一遍 安装任务走一遍 istio 接管了网络，会对部分第三方服务的网络访问造成影响(如：Redis 集群的初始化等)","tags":[{"name":"K8S","slug":"K8S","permalink":"http://www.lockey.xyz/tags/K8S/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://www.lockey.xyz/tags/Kubernetes/"},{"name":"istio","slug":"istio","permalink":"http://www.lockey.xyz/tags/istio/"},{"name":"Server Mesh","slug":"Server-Mesh","permalink":"http://www.lockey.xyz/tags/Server-Mesh/"}]},{"title":"Docker 开源仓库系统 Harbor 搭建与使用","date":"2020-03-12T10:21:45.000Z","path":"2020/03/12/DevOps/Docker开源仓库系统Harbor搭建与使用/","text":"简介 官方 Harbor是一个开源的容器镜像管理系统，它通过基于角色的访问控制来保护镜像，扫描镜像中的漏洞，并将图像签名为可信的。作为一个CNCF孵化项目，Harbor 为您提供了合规性、性能和互操作性，帮助您跨云原生计算平台(如 Kubernetes 和 Docker )一致而安全地管理镜像。 非官方 开源 企业级容器镜像管理系统 成熟 生态健全 使用的大厂众多 搭配在集群内部，加速集群创建、升级 环境 操作系统：Linux(Ubuntu18.04) 容器管理：Docker + Docker-Compose 安装 下载安装包 解压并进入安装包 修改harbor.yml配置（hostname、harbor_admin_password等） 修改docker-compose.yml（如需要修改镜像源等，有条件可以略过） 使用 docker-compose 启动 Harbor 使用 命令行操作与 DockerHUB 基本一致 浏览器 修改 /etc/hosts 文件，设置目标主机IP和 harbor.yml 中设置的 hostname 使用默认用户名 admin 和 harbor.yml 中设置的密码登录 注意事项 使用私有仓库时： 首先需要在 Docker 客户机上登录（$docker login &lt;hostname&gt;） 新版本使用 https，如登录不上： 修改 daemon.json 增加 insecure registries（见下面示例 ↓） 重启 docker 服务 Docker daemon.json 增加 insecure registries vim /etc/docker/daemon.json --- { &quot;registry-mirrors&quot;: [&quot;https://xxx.mirror.aliyuncs.com&quot;], &quot;insecure-registries&quot; : [&quot;&lt;harbor hostname&gt;&quot;] } --- systemctl daemon-reload systemctl restart docker","tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.lockey.xyz/tags/Docker/"},{"name":"Harbor","slug":"Harbor","permalink":"http://www.lockey.xyz/tags/Harbor/"},{"name":"容器","slug":"容器","permalink":"http://www.lockey.xyz/tags/%E5%AE%B9%E5%99%A8/"},{"name":"仓库","slug":"仓库","permalink":"http://www.lockey.xyz/tags/%E4%BB%93%E5%BA%93/"}]},{"title":"Ubuntu18.04 KVM 安装使用（GUI）","date":"2020-03-12T06:22:05.000Z","path":"2020/03/12/Linux/Ubuntu18.04--KVM安装使用/","text":"准备 检查硬件虚拟化支持 $egrep -c '(vmx|svm)' /proc/cpuinfo 输出大于0即可 检查硬件加速 KVM 支持 $apt install cpu-checker $kvm-ok 输出：KVM acceleration can be used即可 桥接模式(可选) 设置宿主机桥接网络，Netplan 模式下的 NetworkManager 管理方式使用 $nm-connection-editor进行管理 安装 KVM $apt install -y qemu qemu-kvm libvirt-bin bridge-utils virt-manager 启动和开机启动 libvirt 服务 $systemctl start libvirtd $systemctl enable libvirtd 使用 KVM 可以使用 $virt-manager GUI 界面管理 安装完系统，设置设置虚拟机 NIC 信息（在虚拟机信息面板）为：指定共享设备，输入宿主机桥接网络名(如：br0)，选择 virtio 即可 博主桥接网络设置静态 IP 没成功，实在不行可以使用 DHCP 自动分发！ KVM 常用命令 命令 | 功能 | 备注 —:---:— virsh list | 查看运行虚拟机 | 查看所有可以加--all参数 未完…待续…","tags":[{"name":"KVM","slug":"KVM","permalink":"http://www.lockey.xyz/tags/KVM/"},{"name":"Ubuntu18.04","slug":"Ubuntu18-04","permalink":"http://www.lockey.xyz/tags/Ubuntu18-04/"}]},{"title":"Linux 基础随笔","date":"2020-03-12T02:58:56.000Z","path":"2020/03/12/Linux/Linux基础随笔/","text":"Linux 与 发行版 发行版 Ubuntu 镜像版本与后缀 版本 版本间具体区别见官网 更新间隔通常为6个月 技术支持至少提供18个月；LTS（Long Time Support）Desktop版本至少3年，Server版本至少5年 版本号通常为 xx.10 Desktop 桌面版：通常指带操作界面的版本（类似 Win 系统和 Mac 系统） Live-Server 服务器版：通常不包含界面，操作靠命令终端，安装时可以选择性安装额外插件或最小化安装。大部分服务器会选择这种类型的镜像。","tags":[{"name":"Linux","slug":"Linux","permalink":"http://www.lockey.xyz/tags/Linux/"},{"name":"基础","slug":"基础","permalink":"http://www.lockey.xyz/tags/%E5%9F%BA%E7%A1%80/"}]},{"title":"高可用K8S搭建（Ubuntu18.04）","date":"2020-03-11T02:53:25.000Z","path":"2020/03/11/DevOps/高可用K8S搭建/","text":"本文资料整理自网络，对其中过时部分进行修正！ 运行环境为：KVM 虚拟机（先根据下文第一部分制作基础镜像，再用基础镜像克隆为各个 master、worker 节点，可以节省时间） 适用版本为： 操作系统：Ubuntu18.04 Kubeadm：GitVersion:&quot;v1.17.3&quot; K8S组件版本如下： kube-apiserver:v1.17.4 kube-controller-manager:v1.17.4 kube-scheduler:v1.17.4 kube-proxy:v1.17.4 pause:3.1 etcd:3.4.3-0 coredns:1.6.5 第一部分----基础镜像准备 Harbor 安装 详见 确保节点之中不可以有重复的主机名、MAC 地址或 product_uuid hostname 修改 Ubuntu：hostnamectl set-hostname xxx 查看 product_uuid sudo cat /sys/class/dmi/id/product_uuid 关闭防火墙（生产环境不建议关闭） systemctl stop ufw systemctl disable ufw 生产环境，建议启用防火墙，并开启 etcd、apiserver等多个端口 关闭 swap 查看 swap：$cat /proc/swaps 注释 swap 设置：$vim /etc/fstab 关闭 swap：$swapoff -a 禁用 GRUB ipv6 编辑：$vim /etc/default/grub 修改： GRUB_CMDLINE_LINUX_DEFAULT=&quot;&quot; GRUB_CMDLINE_LINUX=&quot;&quot; ↓ GRUB_CMDLINE_LINUX_DEFAULT=&quot;ipv6.disable=1&quot; GRUB_CMDLINE_LINUX=&quot;ipv6.disable=1&quot; 更新：$update-grub 设置对应时区（如上海） 执行：$ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 设置 DNS 编辑：$vim /etc/systemd/resolved.conf 修改： # 地址来自阿里 DNS DNS=223.5.5.5 223.6.6.6 升级操作系统内核 参考自青木のJava小屋 查看本机内核：$uname -a 可以根据实际情况选择版本 执行：$dpkg -i xxx.deb升级内核（可使用通配符） 清理旧内核：$dpkg --list | grep linux | awk '{print $2}' | grep 旧内核版本 | xargs apt purge -y 内核操作风险较大，请一步步确认没问题再执行！ 启用 IPVS 内核模块 执行： echo &gt; /etc/modules-load.d/ipvs.conf module=(ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh ip_vs_lc br_netfilter nf_conntrack) for kernel_module in ${module[@]};do /sbin/modinfo -F filename $kernel_module |&amp; grep -qv ERROR &amp;&amp; echo $kernel_module &gt;&gt; /etc/modules-load.d/ipvs.conf || : done 查看：$lsmod | grep ip_vs 输出如下信息为成功 ↓ ip_vs_sh 16384 0 ip_vs_wrr 16384 0 ip_vs_rr 16384 0 ip_vs 147456 6 ip_vs_rr,ip_vs_sh,ip_vs_wrr nf_conntrack 143360 6 xt_conntrack,nf_nat,ipt_MASQUERADE,nf_nat_ipv4,nf_conntrack_netlink,ip_vs libcrc32c 16384 5 nf_conntrack,nf_nat,btrfs,raid456,ip_vs 内核参数调整 cat &gt; /etc/sysctl.conf &lt;&lt; EOF # https://github.com/moby/moby/issues/31208 # ipvsadm -l --timout # 修复ipvs模式下长连接timeout问题 小于900即可 net.ipv4.tcp_keepalive_time = 800 net.ipv4.tcp_keepalive_intvl = 30 net.ipv4.tcp_keepalive_probes = 10 net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 net.ipv6.conf.lo.disable_ipv6 = 1 net.ipv4.neigh.default.gc_stale_time = 120 net.ipv4.conf.all.rp_filter = 0 net.ipv4.conf.default.rp_filter = 0 net.ipv4.conf.default.arp_announce = 2 net.ipv4.conf.lo.arp_announce = 2 net.ipv4.conf.all.arp_announce = 2 net.ipv4.ip_forward = 1 net.ipv4.tcp_max_tw_buckets = 5000 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_max_syn_backlog = 1024 net.ipv4.tcp_synack_retries = 2 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 fs.inotify.max_user_watches=89100 fs.file-max=52706963 fs.nr_open=52706963 net.bridge.bridge-nf-call-arptables = 1 vm.swappiness = 0 vm.max_map_count=262144 EOF 确保 iptables 工具不使用 nftables 后端 切换旧版（Ubuntu） update-alternatives --set iptables /usr/sbin/iptables-legacy update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy update-alternatives --set arptables /usr/sbin/arptables-legacy update-alternatives --set ebtables /usr/sbin/ebtables-legacy docker-ce 安装 安装依赖系统工具：$apt install -y apt-transport-https ca-certificates curl software-properties-common 安装GPG证书：$curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - 写入源：$add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot; 执行：$apt update 安装 docker-ce：$apt install -y docker-ce（另可指定版本号安装指定版本） 设置镜像加速，可以选择阿里云镜像加速服务，详见阿里云镜像加速页面文档 docker-compose 安装 执行： curl -L https://github.com/docker/compose/releases/download/1.25.4/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose Kubeadm 套件安装（国内） 新增源文件：$vim /etc/apt/sources.list.d/kubernetes.list 写入源：$deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main 执行：$apt update 得到错误信息的 key 的末尾8位 执行：$gpg --keyserver keyserver.ubuntu.com --recv-keys 【8位key】 执行：$gpg --export --armor 【8位key】 | sudo apt-key add - 再次执行：$apt update 安装 Kubeadm 套件：$apt install -y kubelet kubeadm kubectl 第二部分----高可用集群搭建 本文高可用模式为：ETCD 高可用 + API-Server 高可用 ETCD：由于 ETCD 使用 Raft 算法，所以节点一般为奇数个，本文使用3个节点 API-Server：本文使用3个节点 Nginx 本地代理 用于访问 master 节点，并在其中某些 master 节点不可用时，自动访问可用节点。 可以部署在每一台 K8S 节点上部署，也可另外部署外部 Nginx 负载均衡节点，本文在每台 K8S 节点上均有部署 配置 master 节点 hosts cat &gt;&gt; /etc/hosts &lt;&lt; EOF 127.0.0.1 server.k8s.local 10.0.0.65 server1.k8s.local 10.0.0.66 server2.k8s.local 10.0.0.67 server3.k8s.local EOF IP 请跟请根据实际情况替换 编写 Nginx 配置文件 mkdir -p /etc/nginx cat &gt; /etc/nginx/nginx.conf &lt;&lt; EOF worker_processes auto; user root; events { worker_connections 20240; use epoll; } error_log /var/log/nginx_error.log info; stream { upstream kube-servers { hash $remote_addr consistent; server server1.k8s.local:6443 weight=5 max_fails=1 fail_timeout=3s; server server2.k8s.local:6443 weight=5 max_fails=1 fail_timeout=3s; server server3.k8s.local:6443 weight=5 max_fails=1 fail_timeout=3s; } server { listen 8443 reuseport; proxy_connect_timeout 3s; # 加大timeout proxy_timeout 3000s; proxy_pass kube-servers; } } EOF 启动 Nginx docker run --restart=always \\ -v /etc/apt/sources.list:/etc/apt/sources.list \\ -v /etc/nginx/nginx.conf:/etc/nginx/nginx.conf \\ --name kps \\ --net host \\ -it \\ -d \\ nginx 高可用外部 ETCD 集群搭建 由于 etcd 不需要对外开放，所以本文不使用 tls 启动参数注意事项： –auto-compaction-retention 由于ETCD数据存储多版本数据，随着写入的主键增加历史版本需要定时清理，默认的历史数据是不会清理的，数据达到2G就不能写入，必须要清理压缩历史数据才能继续写入;所以根据业务需求，在上生产环境之前就提前确定，历史数据多长时间压缩一次;推荐一小时压缩一次数据这样可以极大的保证集群稳定，减少内存和磁盘占用 –max-request-bytes etcd Raft消息最大字节数，ETCD默认该值为1.5M; 但是很多业务场景发现同步数据的时候1.5M完全没法满足要求，所以提前确定初始值很重要;由于1.5M导致我们线上的业务无法写入元数据的问题，我们紧急升级之后把该值修改为默认32M，但是官方推荐的是10M，大家可以根据业务情况自己调整 –quota-backend-bytes ETCD db数据大小，默认是2G，当数据达到2G的时候就不允许写入，必须对历史数据进行压缩才能继续写入;参加1里面说的，我们启动的时候就应该提前确定大小，官方推荐是8G，这里我们也使用8G的配置 在 etcd 集群个节点主机上运行如下命令（注意修改中括号内对应的信息）： mkdir -p /var/etcd docker rm [name] -f rm -rf /var/etcd/* docker run --restart=always --net host -it --name [name] -d \\ -v /var/etcd:/var/etcd \\ -v /etc/localtime:/etc/localtime \\ [仓库名]/etcd:[版本号] \\ etcd --name [etcd name] \\ --auto-compaction-retention &quot;1h&quot; --max-request-bytes &quot;33554432&quot; --quota-backend-bytes &quot;8589934592&quot; \\ --data-dir=/var/etcd/etcd-data \\ --listen-client-urls http://0.0.0.0:2379 \\ --listen-peer-urls http://0.0.0.0:2380 \\ --initial-advertise-peer-urls http://[本节点域名或IP]:2380 \\ --advertise-client-urls http://[本节点域名或IP]:2379,http://[本节点域名或IP]:2380 \\ -initial-cluster-token etcd-cluster \\ -initial-cluster &quot;[本机 etcd name]=http://[本节点域名或IP]:2380,[其他etcd name1]=http://[其他节点域名或IP]:2380,[其他etcd name2]=http://[其他节点域名或IP]:2380&quot; \\ -initial-cluster-state new 测试 etcd 集群 进入正在运行的 etcd 容器：$docker exec -it [name] sh 查看集群状态：$etcdctl --write-out=table --endpoints=&quot;http://[节点域名或IP]:2379,http://[节点域名或IP]:2379,http://[节点域名或IP]:2379&quot; endpoint status 查看集群健康：$etcdctl --write-out=table --endpoints=&quot;http://[节点域名或IP]:2379,http://[节点域名或IP]:2379,http://[节点域名或IP]:2379&quot; endpoint health 测试集群可用性： # 任意节点存入数据 etcdctl put /test/key &quot;123&quot; # 任意节点取出数据，取出正常，集群可用 etcdctl get /test/key API-Server（Master） 高可用 配置自定义 kubeadm-config.yaml 注意修改对应的版本号，仓库路径等 apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration kubernetesVersion: v1.17.0 imageRepository: [仓库路径] apiServer: extraArgs: storage-backend: etcd3 extraVolumes: - hostPath: /etc/localtime mountPath: /etc/localtime name: localtime certSANs: - &quot;prod-server.k8s.local&quot; - &quot;server1.k8s.local&quot; - &quot;server2.k8s.local&quot; - &quot;server3.k8s.local&quot; - &quot;127.0.0.1&quot; - &quot;10.0.0.65&quot; - &quot;10.0.0.66&quot; - &quot;10.0.0.67&quot; - &quot;kubernetes&quot; - &quot;kubernetes.default&quot; - &quot;kubernetes.default.svc&quot; - &quot;kubernetes.default.svc.cluster&quot; - &quot;kubernetes.default.svc.cluster.local&quot; controllerManager: extraArgs: experimental-cluster-signing-duration: 867000h extraVolumes: - hostPath: /etc/localtime mountPath: /etc/localtime name: localtime scheduler: extraVolumes: - hostPath: /etc/localtime mountPath: /etc/localtime name: localtime networking: # pod 网段 podSubnet: 172.224.0.0/12 # SVC 网络 serviceSubnet: 10.96.0.0/12 controlPlaneEndpoint: server.k8s.local:8443 etcd: external: endpoints: - http://server1.k8s.local:2379 - http://server2.k8s.local:2379 - http://server3.k8s.local:2379 --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: ipvs ipvs: scheduler: lc minSyncPeriod: 5s syncPeriod: 15s 初始化 Master 执行：$kubeadm init --config=kubeadm-config.yaml --upload-certs –config=kubeadm-config.yaml 为上一步骤编写的文件 –upload-certs 参数将证书上传到 etcd 中，后续不需要手动分发 初始化结束，控制台输出： Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: 复制并执行下列命令：可以以普通用户操作集群 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run “kubectl apply -f [podnetwork].yaml” with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: 复制并执行下列命令：其余 master 可以加入到到该集群 kubeadm join server.k8s.local:8443 --token uue99t.w9cenfznjoo2e1u6 –discovery-token-ca-cert-hash sha256:afd406ce180ce4c9271c0e7b4c3381df72cd75e766f934fd439d101ae1022a43 –control-plane --certificate-key 1621bb7101d2fa4050779aaa753e89d49ee7db57129b6b4986c82b45fb4dcbe5 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use &quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: 复制并执行下列命令：worker 可以加入到到该集群 kubeadm join server.k8s.local:8443 --token uue99t.w9cenfznjoo2e1u6 –discovery-token-ca-cert-hash sha256:afd406ce180ce4c9271c0e7b4c3381df72cd75e766f934fd439d101ae1022a43 以上命令中出现的 hash、key 需要跟使用者安装后出现的一致，不可直接复制使用！ Token 失效可以使用命令重新生成 ↓ 创建一个新的token： kubeadm token create 查看token列表： kubeadm token list 获取ca证书sha256编码hash值： openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' 最终生成的cmd： kubeadm join server.k8s.local:8443 --token ${TOKEN} --discovery-token-ca-cert-hash sha256:${CaSHA256} master 测试 执行：$kubectl get node 可以查看集群是否加入成功（可以显示加入集群内的所有节点） 执行：$kubectl get pod -A 可以查看所有集群内正在运行的pod（检查 apiserver、controller-manager、proxy、scheduler 是否是 running 状态，coredns 目前为 pending 状态） 结束 至此，高可用 K8S 安装成功。 甜点 集群重置：$kubeadm reset 如果是外接 ETCD ，需要清空外接 ETCD 数据：$etcdctl del &quot;&quot; --prefix 参考链接 在阿里云的VPC部署高可用kubernetes","tags":[{"name":"K8S","slug":"K8S","permalink":"http://www.lockey.xyz/tags/K8S/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://www.lockey.xyz/tags/Kubernetes/"},{"name":"编排","slug":"编排","permalink":"http://www.lockey.xyz/tags/%E7%BC%96%E6%8E%92/"},{"name":"高可用","slug":"高可用","permalink":"http://www.lockey.xyz/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"}]},{"title":"K8S","date":"2019-12-31T06:33:19.000Z","path":"2019/12/31/DevOps/K8S/","text":"部署的演化历史 传统部署 资源分配问题 虚拟化部署 量级较重 容器化部署 敏捷应用程序的创建和部署 持续开发、集成和部署方便 开发运维分离 可观察性丰富 跨开发、测试和生产环境一致 云和操作系统分发的可移植性强 以应用程序为中心管理 适合松散耦合、分布式、弹性、解放的微服务 资源隔离 资源利用率高 容器部署时代面临的问题 问题 服务集群越来越大 传统的集群容器部署方式较繁琐 工作量日益加大 容易出错 解决方向 需要一个强大的容器管理、编排系统 解决方案 kubernetes[K8S] K8S是什么？ 特点 可移植 可扩展 开源 生态系统庞大且有活力 作用 服务发现和负载均衡 存储编排 自动部署和回滚 自动二进制打包 自我修复 秘钥与配置管理 其它意义 促进声明式配置 自动化 架构 架构图 组件 Master【集群控制平面】 kube-apiservice 主节点上负责提供 Kubernetes API 服务的组件 支持水平扩缩 etcd 作为保存 Kubernetes 所有集群数据的后台数据库 兼具一致性和高可用性的键值数据库 通常需要有个备份计划 kube-scheduler 监视新创建的未指定运行节点的 Pod 选择节点让 Pod 在上面运行 kube-controller-manager Node Controller 负责在节点故障时进行通知和响应 Replication Controller（RC） 负责为系统中每个副本控制器对象维护正确数量的 Pod Endpoints Controller 填充端点（Endpoints）对象（即加入 Service 与 Pod） Service Account &amp; Token Controllers 为新的命名空间创建默认账户和 API 访问令牌 cloud-controller-manager 运行与基础云提供商交互的控制器 Node【维护运行的 Pod 并提供 Kubernetes 运行环境】 kubelet 保证容器都运行在 Pod 中 接收一组通过各类机制提供给它的 PodSpecs，确保 PodSpecs 中描述的容器处于运行状态且健康 不会管理不是由 Kubernetes 创建的容器 kube-proxy 是集群中每个节点上运行的网络代理，实现 Kubernetes Service 概念的一部分 维护节点上的网络规则 允许从集群内部或外部网络会话与 Pod 进行网络通信 容器运行环境（Docker 等） 负责运行容器 Addons【使用 Kubernetes 资源 (DaemonSet, Deployment等) 实现集群功能；命名空间为：kube-system】 DNS 所有 Kubernetes 集群都应具有 DNS (非强制性要求) 为 Kubernetes 集群提供 DNS 记录 Kubernetes 启动的容器自动将 DNS 服务器包含在 DNS 搜索中 Dashboard Web UI 使用户可以管理集群中运行的应用程序以及集群本身进行故障排除 容器资源监控 将关于容器的一些常见的时间序列度量值保存到一个集群数据库中 提供用于浏览这些数据的界面 集群层面日志 负责将容器的日志保存到一个集中的日志存储中 提供搜索和浏览接口 Node 其他组件 kubectl【操控 Kubernetes 集群】 负责管理 Pods 和它们上面的容器，镜像、volumes、etc 设计理念 核心理念： 容错性 扩展性 API 设计原则 所有 API 应该是声明式的 AIP 对象是彼此互补且可组合的 高层 API 以操作意图为基础设计 底层 API 根据高层 API 的控制需要设计 尽量避免简单封装，不要有在外部 API 无法显式知道的内部隐藏机制 API 操作复杂度与对象数量成正比 API 对象状态不能依赖于网络连接状态 尽量避免让操作机制依赖于全局状态 控制机制设计原则 控制逻辑应该只依赖于当前状态 假设任何错误的可能，并做容错处理 尽量避免复杂状态机，控制逻辑不要依赖无法监控的内部状态 假设任何操作都可能被任何操作对象拒绝，甚至被错误解析 每个模块都可以在出错后自动恢复 每个模块都可以在必要的时候优雅的降级服务 核心技术概念 术语 语义 备注 Pod 集群中部署应用或服务的最小单元，可以包含多个容器 副本集（Replica Set，RS） 新一代 RC，支持更多匹配模式 部署（Deployment） 表示用户对集群的一次更新操作 服务（Service） 客户端要访问的服务就是 Service 对象 每个 Service 会对应一个集群内部的虚拟 IP，集群内部通过虚拟 IP 访问服务 任务（Job） 控制批处理型任务的对象 后台支撑服务集（DaemonSet） 保证每个节点都有此类 Pod 运行 典型的支持服务有：存储、日志、监控等 有状态服务集（PetSet） 每个 Pod 的名字要事先确定，挂载自己独立的存储 适合 PetSet 的业务：MySQL、PostgreSQL、Zookeeper等 集群联邦（Federation） 提供扩区域跨服务商能力 每个 Federation 有自己的分布式存储、API Server 和 Controller Manager 存储卷（Volume） 与 Docker 存储卷类似，不过作用访问是 Pod 持久存储卷（Persistent Volume，PV）和持久存储卷声明（Persistent Volume Claim，PVC） 使集群具备存储的逻辑抽象能力 PV 和 PVC 的关系相当于 Node 和 Pod 的关系 名称（Names） Kubernetes REST API 中的所有对象都由名称和 UID 明确标识；一次只能有一个给定类型的对象具有给定的名称（如果删除对象，则可以创建同名的新对象） UIDs 系统生成的字符串，唯一标识对象；在集群的整个生命周期中创建的每一个对象都有一个不同的 UID，用来区分类似实体的历史事件 命名空间（Namespace） Kubernetes 支持多个虚拟集群，依赖于同一物理集群。这些虚拟集群被称为命名空间。 标签和选择器（Labels &amp; Selectors） 标签是附加到 Kubernetes 对象上的键值对，用于指定对用户有意义且相关的对象标识属性，但不直接对核心系统有语义含义 每个对象都可以定义一组键/值标签，每个键对于给定对象必须唯一 注解（Annotations） 可以使用注解为对象附加任意的非标识的元数据 字段选择器（Field Selectors） 允许根据多个或多个资源字段的值筛选 Kubernetes 资源 Kubernetes 对象管理 应该只使用一种技术来管理 Kubernetes 对象；混合和匹配技术作用于同一对象上将导致未定义行为 类型 使用环境 优点 缺点 命令式命令 测试 1. 命令简单，易学、易记；2. 快速更改集群 1. 命令不与变更审查流程集成；2. 不提供与更改管理的审核跟踪；3.除了实时内容，命令不提供记录源；4. 不提供用于创建新对象的模板 命令式对象配置 生产 与命令式命令相比：1. 可以存储在源控制系统中，如：Git；2. 可以与流程集成，如：在推送和审计之前检查更新；3.提供用于创建新对象的模板；与声明式对象配置相比：1. 配置行为简单易懂；2. 更加成熟 与命令式命令相比：1. 需要对对象架构有了解；2. 需要些 YAML 文件；与声明式对象配置相比：1. 对活动对象的更新必须反映在配置文件中，否则将在下一次替换时丢失；2. 针对文件而不是目录 声明式对象配置 生产 1. 即使未将对活动对象所做的更改未合并回配置文件中，也将保留这些更改；2. 更好的支持对目录进行操作并自动检测每个对象的操作类型（创建、修补、删除） 1. 配置难于调试并且出现异常时难以理解；2. 使用差异的部分更新会创建复杂的合并和补丁操作 使用 安装 cat &lt;&lt;EOF &gt; /etc/apt/sources.list.d/kubernetes.list deb http://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl apt-mark hold kubelet kubeadm kubectl 参考资料 K8S(官方) Kubernetes 中文社区 | 中文文档 Kubernetes 中文手册","tags":[{"name":"K8S","slug":"K8S","permalink":"http://www.lockey.xyz/tags/K8S/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://www.lockey.xyz/tags/Kubernetes/"},{"name":"编排","slug":"编排","permalink":"http://www.lockey.xyz/tags/%E7%BC%96%E6%8E%92/"}]},{"title":"ELK","date":"2019-12-20T02:00:31.000Z","path":"2019/12/20/DevOps/ELK/","text":"ELK 是什么? 是 ElasticSearch、Logstash、Kibana 三个开源软件的组合 具备数据收集、存储、分析功能 适合实时数据分析和检索的场景 具有分布式特性 易扩展 ELK 功能 ElasticSearch 存储 索引 Logstash 收集 处理 Kibana 可视化 统计分析 Beat（扩展组件） MetricBeat 收集各种主机信息（CPU、内存……），详见官网 Filebeat 轻量级日志收集组件 简单架构 使用 使用 Docker 安装 ELK 参考 https://github.com/deviantony/docker-elk.git Kibana 中文设置 $ vim kibana/config/kibana.yml 新增中文配置 ↓↓↓ i18n.locale: “zh-CN” ElasticSearch 默认需要 4G 内存；可以通过设置 ES_JAVA_OPTS 来修改 如：ES_JAVA_OPTS=&quot;-Xms512m -Xmx512m&quot; 使用 Docker 安装 MetricBeat 获取配置文件 下载 $ wget https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-7.4.1-linux-x86_64.tar.gz 解压 $ tar zxvf metricbeat-7.4.1-linux-x86_64.tar.gz 提取 metricbeat.yml modules.d 使用 Docker Compose $ vim docker-compose.yml version: '2' services: metricbeat: image: docker.elastic.co/beats/metricbeat:7.4.1 container_name: metricbeat volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - /proc:/hostfs/proc:ro - /sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro - /:/hostfs:ro - ./metricbeat/metricbeat.yml:/usr/share/metricbeat/metricbeat.yml:ro - ./metricbeat/modules.d:/usr/share/metricbeat/modules.d:ro - /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro user: &quot;root&quot; command: -system.hostfs=/hostfs -e restart: always docker exec -it metricbeat metricbeat modules enable systemop0=>operation: Data（Log） op1=>operation: MetricBeat op2=>operation: Redis op3=>operation: Logstash op4=>operation: ElasticSearch op5=>operation: Kibana op0(right)->op2 op1(right)->op4 op2(right)->op3 op3(right)->op4 op4(right)->op5{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-0-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-0-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-0\", options);","tags":[{"name":"ELK","slug":"ELK","permalink":"http://www.lockey.xyz/tags/ELK/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.lockey.xyz/tags/ElasticSearch/"},{"name":"Logstash","slug":"Logstash","permalink":"http://www.lockey.xyz/tags/Logstash/"},{"name":"Kibana","slug":"Kibana","permalink":"http://www.lockey.xyz/tags/Kibana/"},{"name":"MetricBeat","slug":"MetricBeat","permalink":"http://www.lockey.xyz/tags/MetricBeat/"},{"name":"日志","slug":"日志","permalink":"http://www.lockey.xyz/tags/%E6%97%A5%E5%BF%97/"}]},{"title":"Zabbix","date":"2019-12-13T07:07:21.000Z","path":"2019/12/13/DevOps/Zabbix/","text":"什么是 Zabbix ？ Zabbix 是一种系统监控。网络监控、服务监控工具。 监控对象 主机 软件 模块功能 Monitoring 信息聚合 状态展示 信息查询 监控统计 Inventory 资产信息 管理 查询 Report 报表 审计 日志 Configuration 主机信息配置 监控规则配置 报警规则配置 统计图表配置 自动发现配置 Administrator 用户权限管理 报警方式配置 脚本配置 代理配置 Personal 接收报警配置 接收报警规则 个人信息管理 Zabbix 特点 安装简单 监控方便 免费开源 自动化 C/S 结构、分布式架构 与同类型比较 系统名称 默认监控 自定义监控 批量监控 修改监控 图表 报警 其他 Zabbix 自带监控多 可自定义编写插件 server 端可配置自动注册规则；client 端无需操作 手动在模板中新增监控 自带图表插件 支持多种报警，依赖插件 自带 Web 监控 Nagios 自带监控少 可自定义编写插件 用脚本在 server 端新增 host， 并拷贝 service 文件 用脚本修改所有主机的 service 文件，加入新增服务 可以按照图表插件 支持多种报警，依赖插件 Zabbix 使用更友好，探索功能需要较多时间，学习成本较大 Nagios 容易上手，脚本很强大，但写起来也费时费力 架构图 部署方式 网络协议模式：server-client 代理模式：server-proxy-client 节点模式：master-node-client 使用 安装 打开官网下载页面 选择操作系统 按引导安装 安装 Server 端 添加源 安装 Server 安装 MySQL 初始化 MySQL 启动 zabbix-server 安装 Web 端 安装 Web 配置 timezone： $ vim /etc/php-fpm.d/zabbix.conf ; php_value[date.timezone] = Asian/Shanghai 启动 [httpd|nginx|apache2] php-fpm 打开 http://[ip]/zabbix 默认用户密码：Admin : zabbix 安装 Agent 端 添加源 安装 Agent 配置连接的 Server $ vim /etc/zabbix/zabbix_agentd.conf 启动 zabbix-agent 踩坑指南 Mariadb 代替 MySQL 关闭 SeLinux，否则可能出现：cannot set resource limit 默认使用 httpd 作为 Web 服务 Web 使用 配置自动发现 配置报警 配置发送报警 配置接收报警 配置统计图表 配置聚合图表 用户管理 配置聚合图表 通用设置套路 Web API 功能 远程管理 Zabbix 配置 远程检索配置和历史数据 接入方式 http://[ip]:[port]/zabbix/api_jsonrpc.php 使用 JSON-RPC 实现 数据传输以 JSON 格式 Headers Content-Type: application/json-rpc Content-Type: application/json Content-Type: application/jsonrequest 接入步骤 登录获取 token 请求头带入 Authorization: [token] 参考资料 中小型企业通用自动化运维架构 zabbix与nagios对比op1=>operation: Configuration op2=>operation: Discovery op3=>operation: Create Discovery Rules op4=>operation: Create Rules op1(right)->op2 op2(right)->op3 op3(right)->op4{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-0-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-0-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-0\", options);op1=>operation: Configuration op2=>operation: Create Action op3=>operation: Create Condition op4=>operation: Create Operation op1(right)->op2 op2(right)->op3 op3(right)->op4{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-1-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-1-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-1\", options);op1=>operation: Administrator op2=>operation: Media Type op3=>operation: Email op4=>operation: Update op1(right)->op2 op2(right)->op3 op3(right)->op4{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-2-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-2-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-2\", options);op1=>operation: Administrator op2=>operation: Users op3=>operation: Admin op4=>operation: Media op5=>operation: Add op1(right)->op2 op2(right)->op3 op3(right)->op4 op4(right)->op5{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-3-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-3-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-3\", options);op1=>operation: Configuration op2=>operation: Hosts op3=>operation: Graphs op4=>operation: 条目选择 op1(right)->op2 op2(right)->op3 op3(right)->op4{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-4-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-4-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-4\", options);op1=>operation: Monitoring op2=>operation: Screens op3=>operation: Add op4=>operation: Edit Screen op1(right)->op2 op2(right)->op3 op3(right)->op4{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-5-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-5-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-5\", options);op1=>operation: Administrator op2=>operation: Users op3=>operation: User Groups op1(right)->op2 op2(right)->op3{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-6-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-6-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-6\", options);op1=>operation: Monitoring op2=>operation: Screens op3=>operation: Add op4=>operation: Edit Screen op1(right)->op2 op2(right)->op3 op3(right)->op4{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-7-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-7-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-7\", options);op1=>operation: 创建监控脚本 op2=>operation: 创建 Zabbix Item op3=>operation: 创建 Zabbix Trigger op4=>operation: 创建 Zabbix Action op1(right)->op2 op2(right)->op3 op3(right)->op4{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-8-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-8-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-8\", options);","tags":[{"name":"运维","slug":"运维","permalink":"http://www.lockey.xyz/tags/%E8%BF%90%E7%BB%B4/"},{"name":"Zabbix","slug":"Zabbix","permalink":"http://www.lockey.xyz/tags/Zabbix/"},{"name":"监控","slug":"监控","permalink":"http://www.lockey.xyz/tags/%E7%9B%91%E6%8E%A7/"}]},{"title":"云计算","date":"2019-12-10T07:43:55.000Z","path":"2019/12/10/DevOps/云计算/","text":"概念 云计算（cloud computing）是分布式计算的一种，指的是通过网络“云”将巨大的数据计算处理程序分解成无数个小程序，然后，通过多部服务器组成的系统进行处理和分析这些小程序得到结果并返回给用户。云计算早期，简单地说，就是简单的分布式计算，解决任务分发，并进行计算结果的合并。因而，云计算又称为网格计算。通过这项技术，可以在很短的时间内（几秒种）完成对数以万计的数据的处理，从而达到强大的网络服务。 现阶段所说的云服务已经不单单是一种分布式计算，而是分布式计算、效用计算、负载均衡、并行计算、网络存储、热备份冗杂和虚拟化等计算机技术混合演进并跃升的结果。 特点 虚拟化(计算、存储、网络等) 产品服务化(IaaS、PaaS、SaaS……) 弹性伸缩 架构 基础设施层(IaaS) 平台层(PaaS) 软件服务层(SaaS) 分类 私有云 公有云 混合云 荤段子论 男人找个女友或老婆是自建私有云，单身约炮或者到娱乐场所消费是公有云服务，按需使用并可弹性扩容，已婚男人找二奶小蜜则属于混合云。","tags":[{"name":"云计算","slug":"云计算","permalink":"http://www.lockey.xyz/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"DevOps","date":"2019-12-09T10:06:13.000Z","path":"2019/12/09/DevOps/DevOps/","text":"前置知识 云计算 概念 DevOps = Development(开发人员) + Operations(运维人员) DevOps 是一种重视“开发人员”和“运维人员”之间沟通合作的文化、运动或者惯例。通过自动化“软件交付”和“架构变更”的流程，使构建、测试、发布软件能更快、更频繁和更可靠。 各部门关系图 对应用程序发布的影响 减少变更范围 消除等待、快速反馈 使问题定位、调试变得简单 加强发布谐调，提升交付质量 自动化，稳定、快速、交付结果可预测 资源利用最大化 关键点 Automated infrastructure（自动化，系统之间的集成） shared version control（SVN共享源码） one step build and deploy（持续构建和部署） feature flags（主干开发） Shared metrics IRC and IM robots（信息整合） CI/CD 一线贯穿以上技术要点 主干开发是 CI 的前提 自动化以及代码周边集中管理是实施 CI 的必要条件 CI/CD 是 DevOps 的技术核心，在没有自动化测试、CI/CD 之下，DevOps 就是空中楼阁 工作流 工具箱 代码管理（SCM） GitHub、GitLab、BitBucket、SubVersion 构建工具 Ant、Gradle、maven 自动部署 Capistrano、CodeDeploy、Ansible、Jenkins 持续集成（CI） Bamboo、Hudson、Jenkins 配置管理 Ansible、Chef、Puppet、SaltStack、ScriptRock GuardRail、Consul、Apollo 容器 Docker、LXC、第三方厂商如AWS 编排 Kubernetes、Core、Apache Mesos、DC/OS 服务注册与发现 Zookeeper、etcd、Consul 脚本语言 python、ruby、shell 日志管理 ELK、Logentries 系统监控 Datadog、Graphite、Icinga、Nagios 性能监控 AppDynamics、New Relic、Splunk 压力测试 JMeter、Blaze Meter、loader.io 预警 PagerDuty、pingdom、厂商自带如AWS SNS HTTP加速器 Varnish 消息总线 ActiveMQ、SQS 应用服务器 Tomcat、JBoss Web服务器 Apache、Nginx、IIS 数据库 MySQL、Oracle、PostgreSQL等关系型数据库；cassandra、mongoDB、redis等NoSQL数据库 项目管理（PM） Jira、Asana、Taiga、Trello、Basecamp、Pivotal Tracker 参考链接 给 DevOps 初学者的入门指南","tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://www.lockey.xyz/tags/Ansible/"},{"name":"自动化","slug":"自动化","permalink":"http://www.lockey.xyz/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"},{"name":"运维","slug":"运维","permalink":"http://www.lockey.xyz/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"Ansible","date":"2019-12-09T09:45:35.000Z","path":"2019/12/09/DevOps/Ansible/","text":"什么是 Ansible ？ 是一种 IT 自动化工具 依赖现有的操作系统的凭证来访问控制远程机器 简单易用、安全可靠 Ansible 可以完成哪些任务？ 系统配置 开发软件 编排高级的 IT 任务（持续集成等） Ansible 的优点 轻量级 易学习 操作灵活 无客户端 推送式 Module 丰富 商业化支持（Tower） Ansible 的缺点 效率低（串行执行） 易挂起 与其他类似项目对比 项目 客户端 通信方式 Web 页面 效率 易用 实现语言 API 开源社区 Ansible 无 ssh 有（Tower） 低 高 Python 有 40.8 K Puppet 有 socket 有 一般 低 Ruby 有 5.6 K Chef 有 ssh 有 一般 高 Python 有 6.1 K Salt 有 RabbitMQ 有 高 高 Python 有 10.5 K 使用 安装 Mac: pip install ansible 配置 Host 配置: /etc/ansible/hosts 配置项: Inventory | Inventory 中文 ssh 配置: ssh-copy-id -i .ssh/[公钥文件] [用户]@[远程主机] Ad-Hoc &amp; Playbook 类似 shell &amp; shell script，但 Playbook 更加强大 Playbook 是对 Ad-Hoc 的编排 Ad-Hoc 适合简单快速的任务 Playbook 适合复杂异步的任务 Playbook 解决 shell script 的编写复杂、操作繁琐、跨平台问题 Ad-Hoc 语法 ansible [远程主机 | 分组] -m [模块] -a '模块参数' Playbook 什么是 Playbook ? 由 YAML 编写 算是一门编程语言 命令集合 功能 声明配置 编排复杂任务 控制任务执行 语法 变量（Inventory &amp; Playbook） vars 流程控制 循环 with_items with_nested (嵌套) 判断 when [true, false, not] 基本结构 host: 被操作的机器 remote_user: 登录机器的用户 tasks: 需要执行的任务 如果语句以 “{” 开头，需要用引号包起来 例子 $ vim test.yml --- - hosts: 192.168.1.111 // 指定主机 remote_user: root // 指定在被管理的主机上执行任务的用户 tasks: // 任务列表 - name: disable selinux // 任务名 command: '/sbin/setenforce 0' // 调用command模块 执行关闭防火墙命令 - name: start nginx // 任务名 service: name=nginx state=started // 调用service模块 开启 nginx 服务 $ ansible-playbook test.yml","tags":[{"name":"Ansible","slug":"Ansible","permalink":"http://www.lockey.xyz/tags/Ansible/"},{"name":"自动化","slug":"自动化","permalink":"http://www.lockey.xyz/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"},{"name":"运维","slug":"运维","permalink":"http://www.lockey.xyz/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"弹性伸缩","date":"2019-12-09T06:37:59.000Z","path":"2019/12/09/Aliyun/弹性伸缩/","text":"能力 根据业务需求和策略自动调整弹性计算资源，在业务需求增长时无缝增加 ECS 实例满足计算需要，在业务需求下降时自动减少 ECS 实例节约成本。 使用 步骤 op1=>operation: 创建伸缩组 op2=>operation: 创建伸缩配置 op3=>operation: 启用伸缩组 op4=>operation: 创建伸缩规则 op5=>operation: 创建定时任务 op6=>operation: 创建报警任务 op1(right)->op2 op2(right)->op3 op3(right)->op4 op4(right)->op5 op5(right)->op6{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-0-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-0-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-0\", options);","tags":[{"name":"弹性伸缩","slug":"弹性伸缩","permalink":"http://www.lockey.xyz/tags/%E5%BC%B9%E6%80%A7%E4%BC%B8%E7%BC%A9/"}]},{"title":"Docker-部署-Jenkins","date":"2019-12-09T01:19:13.000Z","path":"2019/12/09/DevOps/Docker-部署-Jenkins/","text":"部署 Dockerfile FROM jenkins/jenkins:lts MAINTAINER Lockey USER root ARG dockerGid=999 RUN echo &quot;docker:x:${dockerGid}:jenkins&quot; &gt;&gt; /etc/group 构建 docker build . -t myjenkins 运行 docker run -d --privileged=true -p 8005:8080 -p 50000:50000 --name myjenkins_1 -v /root/jenkins:/var/jenkins_home -v /root/.ssh:/var/jenkins_home/.ssh -v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/docker:/var/lib/docker -v $(which docker):/usr/bin/docker -v $(which docker-compose):/usr/bin/docker-compose myjenkins Slave 安装Java(8) apt install openjdk-8-jre-headless 配置免密登录 ssh-keygen -t rsa 注意 .ssh文件挂载 Git访问 Slave访问 Dockerfile 将用户加入docker用户组，不然没有Docker访问权限 更换源【国内】 打开： http[s]://[URL]/pluginManager/advanced 替换升级站点URL：https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json PS：如果出现更换后还是走了官方源，可以配置 Nginx： rewrite ^/download/plugins/(.*)$ https://mirrors.tuna.tsinghua.edu.cn/jenkins/plugins/$1? last;","tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://www.lockey.xyz/tags/Jenkins/"}]},{"title":"Gitbook","date":"2019-12-05T03:58:04.000Z","path":"2019/12/05/Gitbook/","text":"Dockerfile FROM node WORKDIR /build EXPOSE 4000 RUN npm install gitbook -g; RUN npm install gitbook-cli -g; WORKDIR /build/work RUN gitbook init CMD [&quot;gitbook&quot;, &quot;serve&quot;] 构建 $ docker build . -t acgitbook 运行 $ docker run -d --name acgitbook_1 -p 8006:4000 -v /root/jenkins/workspace/acdocs/work:/build/work acgitbook /bin/bash -c &quot;gitbook install;gitbook serve&quot;","tags":[{"name":"文档工具","slug":"文档工具","permalink":"http://www.lockey.xyz/tags/%E6%96%87%E6%A1%A3%E5%B7%A5%E5%85%B7/"}]}]